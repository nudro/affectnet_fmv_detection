{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import dlib\n",
    "import imgaug as ia\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Load the Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2) #was Max\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=2, stride=1, padding=1)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 184, kernel_size=2, stride=1, padding=0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(184 * 24 * 24, 1000)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1000, 50)\n",
    "        \n",
    "        self.fc3 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(-1, 184 * 24 * 24)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PATH = './fear5.pth'\n",
    "#torch.save(net.state_dict(), PATH)\n",
    "\n",
    "net = Net()\n",
    "\n",
    "#net = torch.load(\"fear5.pth\")\n",
    "net.load_state_dict(torch.load(\"fear5.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(128, 184, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=105984, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prediction on single images</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = \"fear4.jpg\"\n",
    "frame = cv2.imread(img, flags=cv2.IMREAD_COLOR)\n",
    "\n",
    "cv2.imshow('image',frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize frame for speed.\n",
    "frame = cv2.resize(frame, (400,200), interpolation=cv2.INTER_CUBIC) #resize #originally INTER_CUBIC\n",
    "frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "frame_rgb = cv2.cvtColor(frame_gray, cv2.COLOR_GRAY2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_face = \"haarcascade_frontalface_alt.xml\"\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(haar_face)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector() #detects faces\n",
    "\n",
    "nfaces = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_processor(frame_rgb, x, y, w, h):\n",
    "    im_size = 100\n",
    "    horizontal_offset = 0 \n",
    "    vertical_offset = 0 \n",
    "    extracted_face = frame_rgb[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n",
    "    extracted_face_r = cv2.resize(extracted_face, (im_size, im_size)) #resize the extracted face\n",
    "    extracted_face_arr = np.array(extracted_face_r)\n",
    "    normalized = np.array(extracted_face_arr, np.float32) / 255.\n",
    "    normalized_arr = np.expand_dims(normalized, axis=0) #only one pic, one sample, so expand dims to turn into 4D array\n",
    "    face_torch = torch.from_numpy(normalized_arr) #pytorch will only accept torch tensors\n",
    "    face_torch = face_torch.reshape(1, 3, im_size, im_size) #need to reshape from (1, 100,100, 3) to (1, 3, 100, 100)\n",
    "    face_torch = face_torch.to(device)\n",
    "    return face_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(test_image):\n",
    "    with torch.no_grad():\n",
    "        net.cuda()\n",
    "        output = net(test_image)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        sm = torch.nn.Softmax()\n",
    "        probabilities = sm(output) \n",
    "        fear_probs = probabilities[:, 0]\n",
    "        neutral_probs = probabilities[:, 1]\n",
    "        return fear_probs.item(), neutral_probs.item(), predicted.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.748320396989584e-07 0.9999991655349731 1\n",
      "0.00120182940736413 0.998798131942749 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/catherine/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "faces = faceCascade.detectMultiScale(frame_rgb, scaleFactor=1.1, minNeighbors=3, minSize=(20, 20), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "nfaces=0\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "    nfaces = nfaces + 1\n",
    "    extracted_array = img_processor(frame_rgb, x, y, w, h) \n",
    "    extracted_array.to(device)\n",
    "    p_fear, p_neutral, predicted_class = prediction(extracted_array.to(device)) #apply img_processor func: extracts face and resizes, converts to torch\n",
    "    print(p_fear, p_neutral, predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = \"fear4.jpg\"\n",
    "im_size = 400\n",
    "\n",
    "\n",
    "image = cv2.imread(item, flags=cv2.IMREAD_COLOR) #open image\n",
    "face = ia.imresize_single_image(image, (im_size, im_size)) #resize\n",
    "gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "rgb = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "detected_faces = faceCascade.detectMultiScale(rgb, scaleFactor=1.1, minNeighbors=3, minSize=(50, 50), flags=cv2.CASCADE_SCALE_IMAGE) #detect faces\n",
    "\n",
    "\n",
    "for face in detected_faces:\n",
    "    (X, Y, w, h) = face\n",
    "    if w > 20:\n",
    "        horizontal_offset = 0\n",
    "        vertical_offset = 0\n",
    "        extracted_face = rgb[Y+vertical_offset:Y+h, X+horizontal_offset:X-horizontal_offset+w]\n",
    "        extracted_face_r = ia.imresize_single_image(extracted_face, (im_size, im_size)) #resize\n",
    "        \n",
    "        tester = img_processor(extracted_face_r, X, Y, w, h)\n",
    "        tester.to(device)\n",
    "        prediction = prediction(tester.to(device))\n",
    "    \n",
    "        print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_face = \"haarcascade_frontalface_alt.xml\"\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(haar_face)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector() #detects faces\n",
    "\n",
    "faces = faceCascade.detectMultiScale(frame_rgb, 1.1, scaleFactor=1.1, minNeighbors=3, minSize=(20, 20), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "nfaces = 0\n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "    nfaces = nfaces + 1\n",
    "        \n",
    "    extracted_array = img_processor(frame_rgb, x, y, w, h) #apply img_processor func: extracts face and resizes, converts to array\n",
    "        \n",
    "    prediction_result = prediction(extracted_array)\n",
    "    \n",
    "    prediction_class = prediction_result.argmax(axis=-1)\n",
    "    \n",
    "    print(prediction_result)\n",
    "\n",
    "# show result\n",
    "cv2.imshow(\"Result\",frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
